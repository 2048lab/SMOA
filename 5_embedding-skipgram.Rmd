---
title: "Skigram-tidy"
author: "Jilung Hsieh"
date: "`r Sys.Date()`"
output: html_document
---

<https://juliasilge.com/blog/tidy-word-vectors/> for skipgram idea <https://juliasilge.com/blog/word-vectors-take-two/> for glove idea <https://juliasilge.github.io/why-r-webinar/#81> <https://www.r-bloggers.com/2020/08/whats-the-difference-between-instagram-and-tiktok-using-word-embeddings-to-find-out-2/>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r concept}
DiagrammeR::grViz("
digraph G {
  graph [layout = dot, rankdir=TD]
  node [shape = rect,  height=0, fontname='Helvetica', width=2]
  node [style = filled, fillcolor='honeydew1']
  edge [fontname='Courier']
  
    corpus[fillcolor='khaki3']
    corpus -> {ngram_8, dtlist}
    dtlist -> dtcount
    dtcount -> unigramProb
    ngram_8[labels='quanteda::\ntokens_ngrams()']
    ngram_8 -> skipgram[label='unnest()']
    skipgram -> skipgram_ttpair[label='pairwise_count()']
    normalized_ttpair[shape='octagon', fillcolor='pink1']
    {unigramProb, skipgram_ttpair} -> normalized_ttpair
    normalized_ttpair -> pmi_matrix[label='cast_sparse(w1, w2, pmi)']
    sim_crossprod[shape=rect, fillcolor=lightblue1]
    svd_pmi[fillcolor=yellow]
    pmi_matrix -> svd_pmi -> sim_crossprod
}
")
```


# Julia's v

```{r}
library(tidyverse)
library(tidytext)
library(janeaustenr)

austen_text <- austen_books() %>%
    mutate(text = na_if(text, "")) %>%
    drop_na(text) %>%
    mutate(postID = row_number())
```

```{r unigram-prob}
library(tidytext)

unigram_probs <- austen_text %>%
    unnest_tokens(word, text) %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n))

```

```{r skipgram}
library(widyr)

tidy_skipgrams <- austen_text %>%
    unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
    drop_na(ngram) %>%
    mutate(ngramID = row_number()) %>%
    unite(skipgramID, postID, ngramID) %>% 
    unnest_tokens(word, ngram)

tidy_skipgrams
```

```{r skipgram-probs}
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))
```

```{r normalized-skip-gram-probs}
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

```

```{r}
normalized_prob %>% 
    filter(word1 == "father") %>%
    arrange(-p_together)
```

```{r cast-sparse}
pmi_matrix <- normalized_prob %>%
    mutate(pmi = log10(p_together)) %>%
    cast_sparse(word1, word2, pmi)

```

```{r Dim-Reduce}
library(irlba)
pmi_svd <- irlba(pmi_matrix, 256, maxit = 1e3)
```

```{r Retrieve-Converting-Matrix}
word_vectors <- pmi_svd$u
# rownames(pmi_matrix)
rownames(word_vectors) <- rownames(pmi_matrix)
```

```{r search-func}
library(broom)

search_synonyms <- function(word_vectors, selected_vector) {
    
    # similarities <- word_vectors %*% selected_vector %>%
    #     tidy() %>%
    #     as_tibble() %>%
    #     rename(token = .rownames,
    #            similarity = unrowname.x.)
    similarities <- word_vectors %*% selected_vector %>% 
        as.data.frame() %>%
        rownames_to_column(var = "token") %>%
        rename(similarity = V1)
    
    similarities %>%
        arrange(-similarity)    
}
```

```{r search-synonyms}
search_synonyms(word_vectors, word_vectors["father",]) %>%
    slice(1:20)
# word_vectors["father",]
```

