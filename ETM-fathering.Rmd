---
title: "word2vec"
author: "Jilung Hsieh"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    highlight: zenburn
    fig_width: 8
    fig_height: 4
    fig_caption: yes
    df_print: tibble
    params:
        output_dir:"html"
---



# wordVectors
* https://github.com/bmschmidt/wordVectors
* We will use a `wordVector` package, installing by  `devtools::install_github("bmschmidt/wordVectors")`
* `rword2vec` is another packages for building word2vec model

Raw document
 -> doc_id - word
 -> tokenized (text_data.txt)
 -> wordVectors::train_word2vec (vec.bin)

vec.bin -> word2vec
    | model     <- read.word2vec("vec.bin")
    | terms     <- summary(model, "vocabulary")
    | embedding <- as.matrix(model)

```{r}
knitr::opts_chunk$set(echo = TRUE, cache = T)
```


```{r load data}
load("data/s3_watched.rda")
Sys.setlocale(locale="zh_TW.UTF-8")
```


```{r loading-libraries}
library(tidyverse)
library(stringr)
library(tidytext)
library(jiebaR)
library(lubridate)
# devtools::install_github("bmschmidt/wordVectors")
library(wordVectors)
library(word2vec)
library(tidyr) 
options(scipen = 999)

```




```{r initial-jieba}
library(jiebaR)
stopWords <- readRDS("data/stopWords.rds")
segment_not <- c("爸爸", "爸媽", "新手")
watched <- c("爸爸","父親","老公","先生","丈夫","奶爸",
             "寶爸","隊友","爹地","爸比","把拔","把鼻",
             "老爸","另一半","拔拔", "孩子的爸","孩子爸",
             "爸拔","他爸","她爸","新手爸","版爸", "板爸",
             "我家男人","當爸的","腦公","阿爸","人父",
             "孩子的爹","孩子爹","老爹","外子","拔比",
             "爸鼻","爸把","爸逼","爸咪","把爸","拔爸",
             "爹低","帥爸","準爸","小孩爸","親爸","神爸",
             "宅爸","瓶餵爸","寶寶的爸","孩的爸","女兒的爸")

reserved <- c("神隊友", "豬隊友", "好隊友", "好先生", "好爸爸", "好老公")

watched <- c(watched, reserved)

watched.str <- paste0(watched, collapse = "|")


reserved <- c("神隊友", "豬隊友", "好隊友", "好先生", "好爸爸", "好老公")

cutter <- worker()
tagger <- worker("tag")
new_user_word(cutter, segment_not)
new_user_word(cutter, watched)
new_user_word(tagger, segment_not)
new_user_word(tagger, watched)
```


# Tokenization
```{r tokenization}
unnested.df <- s3.watched %>%
    # filter(str_detect(sentence, "隊友")) %>%
    mutate(word = purrr::map(s3, function(x)segment(x, cutter))) %>%
    unnest(word) %>%
    anti_join(stopWords) %>%
    filter(!str_detect(word, "[a-zA-Z0-9]+")) %>%
    filter(!is.na(word)) %>%
    group_by(word) %>%
    filter(n() > 5) %>%
    ungroup() %>%
    filter(nchar(word) > 1)
```


```{r tokenized and save to text}
id2text <- unnested.df %>% 
    select(doc_id = sid, word) %>%
	group_by(doc_id) %>%
	summarise(text = paste(word, collapse = " ")) %>%
	ungroup()

id2text %>%
    .$text %>%
	write("text_data.txt")
```

# EMBEDDINGS

## word2vec::Training Model
* Once you trained a model, the model can be saved to a `vec.bin` binary file. If you need to use the model again, you needn't rebuild a model, just load the model by `read.vectors(file_name)`.

```{r}
model = train_word2vec("text_data.txt", output="vec100.bin",
                       threads = 4, vectors = 100,
                       window =5, min_count = 12,
                       iter=10, force=TRUE)


# model = read.vectors("vec.bin")
```

## read by word2vec
```{r}
model <- read.word2vec("vec100.bin")
terms <- summary(model, "vocabulary")
embedding <- as.matrix(model)
```


# ETM
```{r}
dtm   <- strsplit.data.frame(id2text, group = "doc_id", term = "text", split = " ")
dtm   <- document_term_frequencies(dtm)
dtm   <- document_term_matrix(dtm)
dtm   <- dtm_remove_tfidf(dtm, prob = 0.50)

vocab        <- intersect(rownames(embedding), colnames(dtm))
embedding   <- dtm_conform(embedding, rows = vocab)
dtm          <- dtm_conform(dtm,     columns = vocab)

dim(dtm)
dim(embedding)
```




```{r}
library(topicmodels.etm)

set.seed(1234)
torch_manual_seed(4321)

model <- ETM(k = 20, 
             dim = 100, 
             embeddings = embedding)

optimizer <- optim_adam(params = model$parameters, 
                        lr = 0.005, 
                        weight_decay = 0.0000012)

loss <- model$fit(data = dtm, 
                  optimizer = optimizer, 
                  epoch = 20, 
                  batch_size = 1000)
plot(model, type = "loss")

```
## d. Inspect the model

```{r}
terminology  <- predict(model, type = "terms", top_n = 10)

topic.df <- 1:20 %>%
    purrr::map(function(i){terminology[[i]] %>% mutate(topic=i)}) %>%
    bind_rows()

topic.df %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot() + aes(term, beta, fill=factor(topic)) + 
    geom_col(show.legend = FALSE) +
	facet_wrap(~ topic, scales = "free") +
    coord_flip() + 
    theme_minimal() + 
    theme(text = element_text(family = "Heiti TC Light"))


```
