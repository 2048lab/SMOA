---
title: 'ETM: TM with embeddings'
author: "Jilung Hsieh"
date: "`r Sys.Date()`"
output: html_document
---

https://github.com/bnosac/ETM

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# pkgs <- c("torch", "topicmodels.etm", "word2vec", "doc2vec", "udpipe", "uwot")
# install.packages(pkgs)
library(torch)
library(topicmodels.etm)
library(tidyverse)
```

```{r pkgs-to-plot-the-model, eval=FALSE, include=FALSE}
install.packages("textplot")
install.packages("ggrepel")
install.packages("ggalt")
```


## a. Get data

```{r}
library(torch)
library(topicmodels.etm)
library(doc2vec)
library(word2vec)
data(be_parliament_2020, package = "doc2vec")
x      <- data.frame(doc_id           = be_parliament_2020$doc_id, 
                     text             = be_parliament_2020$text_nl, 
                     stringsAsFactors = FALSE)
x$text <- txt_clean_word2vec(x$text)
```


## b. Build a word2vec model to get word embeddings

```{r}
w2v        <- word2vec(x = x$text, dim = 25, type = "skip-gram", iter = 10, min_count = 5, threads = 2)
embeddings <- as.matrix(w2v)

predict(w2v, newdata = c("migranten", "belastingen"), type = "nearest", top_n = 4)
```

## c. Build the embedding topic model
```{r prepare-dtm}
library(udpipe)
dtm   <- strsplit.data.frame(x, group = "doc_id", term = "text", split = " ")
dtm   <- document_term_frequencies(dtm)
dtm   <- document_term_matrix(dtm)
dtm   <- dtm_remove_tfidf(dtm, prob = 0.50)

vocab        <- intersect(rownames(embeddings), colnames(dtm))
embeddings   <- dtm_conform(embeddings, rows = vocab)
dtm          <- dtm_conform(dtm,     columns = vocab)
dim(dtm)
dim(embeddings)
```
```{r modeling}
set.seed(1234)
torch_manual_seed(4321)

model <- ETM(k = 20, 
             dim = 100, 
             embeddings = embeddings)

optimizer <- optim_adam(params = model$parameters, 
                        lr = 0.005, 
                        weight_decay = 0.0000012)

loss <- model$fit(data = dtm, 
                  optimizer = optimizer, 
                  epoch = 20, 
                  batch_size = 1000)
plot(model, type = "loss")
```

## d. Inspect the model

```{r}
terminology  <- predict(model, type = "terms", top_n = 10)

topc.df <- 1:20 %>%
    purrr::map(function(i){terminology[[i]] %>% mutate(topic=i)}) %>%
    bind_rows()

```


## e. Predict alongside the model
```{r}
newdata <- head(dtm, n = 5)
scores  <- predict(model, newdata, type = "topics")
scores
```

## f. Save / Load model
```{r}
torch_save(model, "example_etm.ckpt")
model <- torch_load("example_etm.ckpt")
```

## g. Optionally - visualise the model in 2D
```{r}
library(textplot)
library(uwot)
library(ggrepel)
library(ggalt)
manifolded <- summary(model, type = "umap", n_components = 2, 
                      metric = "cosine", n_neighbors = 15, 
                      fast_sgd = FALSE, n_threads = 2, verbose = T)
space <- subset(manifolded$embed_2d, type %in% "centers")

textplot_embedding_2d(space)
space <- subset(manifolded$embed_2d, 
                cluster %in% c(12, 14, 9, 7) & rank <= 7)
textplot_embedding_2d(space, 
                      title = "ETM topics", 
                      subtitle = "embedded in 2D using UMAP", 
                      encircle = FALSE, points = TRUE)
```

