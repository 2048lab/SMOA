
```{r}
library(tidytext)
library(tidyverse)
# options(stringsAsFactors = F)

# install.packages("text2vec")
library(text2vec)
# browseURL("http://text2vec.org/glove.html")
```



# download the same Wikipedia data used as a demo by word2vec:

```{r}
text8_file = "~/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "~/text8.zip")
  unzip ("~/text8.zip", files = "text8", exdir = "~/")
}
wiki = readLines(text8_file, n = 1, warn = FALSE)
```



# create a vocabulary
* create a vocabulary, a set of words for which we want to learn word vectors. Note, that all of text2vecâ€™s functions which operate on raw text data (create_vocabulary, create_corpus, create_dtm, create_tcm) have a streaming API and you should iterate over tokens as the first argument for these functions.

```{r}
# Create iterator over tokens
tokens = space_tokenizer(wiki)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
class(vocab)
class(it)
class(tokens)
```



* These words should not be too uncommon. Fot example we cannot calculate a meaningful word vector for a word which we saw only once in the entire corpus. Here we will take only words which appear at least five times. text2vec provides additional options to filter vocabulary (see ?prune_vocabulary).
```{r}
vocab = prune_vocabulary(vocab, term_count_min = 5L)
```

# Construct Term-Co-occurrence Matrix (TCM)
* Now we have 71,290 terms in the vocabulary and are ready to construct term-co-occurence matrix (TCM).
* tcm is a large `dgTMatrix` with size 5,082,264,100 elements
* dgTMatrix is belonging to Matrix package https://www.rdocumentation.org/packages/Matrix/versions/0.95-1
```{r}
# Use our filtered vocabulary
vectorizer = vocab_vectorizer(vocab)
# use window of 5 for context words
tcm = create_tcm(it, vectorizer, skip_grams_window = 5L)
```

```{r}
glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
```


```{r}
wv_context = glove$components
dim(wv_context)
```


```{r}
word_vectors = wv_main + t(wv_context)
```


```{r}
berlin = word_vectors["paris", , drop = FALSE] - 
  word_vectors["france", , drop = FALSE] + 
  word_vectors["germany", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = berlin, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```


# Chinese-Taiwanese
```{r}

```


